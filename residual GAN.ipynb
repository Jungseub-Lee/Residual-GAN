{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters, kernel_size=3, stride=1):\n",
    "    shortcut = x\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.add([x, shortcut])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    inputs = layers.Input(shape=(512, 512, 3))\n",
    "    x = layers.Conv2D(64, 7, padding='same')(inputs)\n",
    "    x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "\n",
    "    # Downsample\n",
    "    x = layers.Conv2D(128, 3, strides=2, padding='same')(x)\n",
    "    x = layers.Conv2D(256, 3, strides=2, padding='same')(x)\n",
    "\n",
    "    # Residual blocks\n",
    "    for _ in range(6):\n",
    "        x = residual_block(x, 256)\n",
    "\n",
    "    # Upsample\n",
    "    x = layers.Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    x = layers.Conv2D(3, 7, activation='tanh', padding='same')(x)\n",
    "    return models.Model(inputs, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    inputs = layers.Input(shape=(512, 1024, 3))\n",
    "    x = layers.Conv2D(64, 4, strides=2, padding='same')(inputs)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 4, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Conv2D(256, 4, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Conv2D(512, 4, strides=2, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(1, activation='sigmoid')(x)\n",
    "    return models.Model(inputs, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Compile discriminator\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# GAN model\n",
    "discriminator.trainable = False\n",
    "gan_input = layers.Input(shape=(512, 512, 3))\n",
    "generated_image = generator(gan_input)\n",
    "gan_output = discriminator(tf.concat([gan_input, generated_image], axis=2))\n",
    "gan = models.Model(gan_input, gan_output)\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, [512, 512])\n",
    "    image = (image - 127.5) / 127.5  # Normalize the image to [-1, 1]\n",
    "    return image\n",
    "\n",
    "def prepare_datasets(brightfield_paths, fluorescent_paths, train_size=0.7, val_size=0.2):\n",
    "    # Split datasets\n",
    "    total_images = len(brightfield_paths)\n",
    "    train_end = int(train_size * total_images)\n",
    "    val_end = int((train_size + val_size) * total_images)\n",
    "\n",
    "    train_bf = brightfield_paths[:train_end]\n",
    "    val_bf = brightfield_paths[train_end:val_end]\n",
    "    test_bf = brightfield_paths[val_end:]\n",
    "\n",
    "    train_fl = fluorescent_paths[:train_end]\n",
    "    val_fl = fluorescent_paths[train_end:val_end]\n",
    "    test_fl = fluorescent_paths[val_end:]\n",
    "\n",
    "    # Load and batch datasets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_bf, train_fl))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_bf, val_fl))\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((test_bf, test_fl))\n",
    "\n",
    "    # Apply preprocessing and batching\n",
    "    train_ds = train_ds.map(lambda x, y: (load_and_preprocess_image(x), load_and_preprocess_image(y))).batch(1)\n",
    "    val_ds = val_ds.map(lambda x, y: (load_and_preprocess_image(x), load_and_preprocess_image(y))).batch(1)\n",
    "    test_ds = test_ds.map(lambda x, y: (load_and_preprocess_image(x), load_and_preprocess_image(y))).batch(1)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(gan, generator, discriminator, train_dataset, val_dataset, epochs=50):\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        # Training\n",
    "        for train_x, train_y in train_dataset:\n",
    "            # Training step code here\n",
    "\n",
    "        # Validation\n",
    "        for val_x, val_y in val_dataset:\n",
    "            # Validation step code here\n",
    "\n",
    "        # Record the average losses for plotting\n",
    "        history['train_loss'].append(np.mean(train_losses))\n",
    "        history['val_loss'].append(np.mean(val_losses))\n",
    "\n",
    "        # Visualization of the loss trend every epoch\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(history['train_loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Loss Trend')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Optionally save model every few epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            generator.save(f'generator_epoch_{epoch+1}.h5')\n",
    "            discriminator.save(f'discriminator_epoch_{epoch+1}.h5')\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_data('/path/to/brightfield', '/path/to/fluorescent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataset is already loaded and prepared\n",
    "train(gan, generator, discriminator, train_dataset, val_dataset, epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
